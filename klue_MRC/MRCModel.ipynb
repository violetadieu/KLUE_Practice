{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedbf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLUE QA Dataset과 Roberta-large모델을 활용한 학습 예제 입니다.\n",
    "#Model을 만드는 코드\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering,AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a80763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data를 읽어서 각 feature 별로 전처리하는 함수\n",
    "def readTrainData(path):\n",
    "    with open(path,'rb')as file:\n",
    "        MRCdata=json.load(file)\n",
    "    #data 구성\n",
    "    #context - 문장\n",
    "    #question - 질문\n",
    "    #answer - 정답\n",
    "    print(MRCdata)\n",
    "    contexts=list()\n",
    "    \n",
    "    questions=list()\n",
    "    answers=list()\n",
    "    \n",
    "    #하나의 문장에 여러 질문이 있을 수 있고, 질문이 여러개면 답변도 여러개이기 때문에 4중for문 사용\n",
    "    for item in tqdm(MRCdata[\"data\"]):\n",
    "        for passage in item['paragraphs']:\n",
    "            context=passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question=qa['question']\n",
    "                for ans in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(ans)\n",
    "    return contexts,questions,answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb9fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset에 정답의 끝나는 index도 추가하는 함수\n",
    "def endIdx(answers,contexts):\n",
    "    for answer,context in zip(answers,contexts):\n",
    "        ansText=answer['text']\n",
    "        startIdx=answer['answer_start']\n",
    "        endIdx=startIdx+len(ansText)\n",
    "        \n",
    "        answer['answer_end']=endIdx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82df87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KlueMRCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, contexts, questions, answers, modelMaxPositionEmbedings, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.answers = answers\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.model_max_position_embedings = modelMaxPositionEmbedings\n",
    "        self.encodings = self.tokenizer(self.contexts, \n",
    "                                        self.questions,\n",
    "                                        max_length=512,\n",
    "                                        truncation=True,\n",
    "                                        padding=\"max_length\",\n",
    "                                        return_token_type_ids=False)\n",
    "        self.addTokenPositions()\n",
    "        \n",
    "    def addTokenPositions(self):\n",
    "        startPositions = []\n",
    "        endPositions = []\n",
    "        for i in range(len(self.answers)):\n",
    "            startPositions.append(self.encodings.char_to_token(i, self.answers[i]['answer_start']))\n",
    "            endPositions.append(self.encodings.char_to_token(i, self.answers[i]['answer_end'] - 1))\n",
    "\n",
    "            # positions 값이 None 값이라면, answer가 포함된 context가 잘렸다는 의미\n",
    "            if startPositions[-1] is None:\n",
    "                startPositions[-1] = self.model_max_position_embedings\n",
    "            if endPositions[-1] is None:\n",
    "                endPositions[-1] = self.model_max_position_embedings\n",
    "\n",
    "        self.encodings.update({'startPositions': startPositions, 'endPositions': endPositions})\n",
    "\n",
    "        \n",
    "    def get_data(self):\n",
    "        return {\"contexts\":self.contexts, 'questions':self.questions, 'answers':self.answers}\n",
    "    \n",
    "    \n",
    "    def get_encodings(self):\n",
    "        return self.encodings\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ca9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저 가저오기\n",
    "tokenizerName=\"klue/bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizerName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31e44c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model 가저오기\n",
    "modelName=\"klue/bert-base\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491253ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 data 전처리\n",
    "#aihubData\n",
    "dataName=\"./dataset/ko_nia_normal_squad_all.json\"\n",
    "\n",
    "#KLUE-Data\n",
    "#dataName=\"./dataset/klue-mrc-v1.1_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efeaea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f82d72578e346da85b7e2e9d3be4ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contexts,questions,answers=readTrainData(dataName)\n",
    "endIdx(answers,contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a92d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=KlueMRCDataset(contexts,questions,answers,512,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6e32bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼파라미터 정의\n",
    "EPOCH=3\n",
    "LEARNING_RATE=5e-5\n",
    "BATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e64c2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 훈련 실행함수(AdamW사용)\n",
    "#좀 더 다양한 dataset을 활용해 여러 모델을 만들 예정이므로 각 모델의 이름을 구별하기 위해 modelName변수를 활용\n",
    "def train_runner(model, dataset, batch_size, num_train_epochs, learning_rate):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "    global_total_step = len(train_dataloader) * num_train_epochs\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    with tqdm(total=global_total_step, unit='step') as t:\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        for epoch in range(num_train_epochs):\n",
    "            for batch in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                startPositions = batch['startPositions'].to(device)\n",
    "                endPositions = batch['endPositions'].to(device)\n",
    "                outputs = model(input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             start_positions=startPositions,\n",
    "                             end_positions=endPositions)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_loss = loss.item() * len(input_ids)\n",
    "                total += len(input_ids)\n",
    "                total_loss += batch_loss\n",
    "                global_total_step += 1\n",
    "                t.set_postfix(loss=\"{:.6f}\".format(total_loss / total), batch_loss=\"{:.6f}\".format(batch_loss))\n",
    "                t.update(1)\n",
    "                \n",
    "                del input_ids\n",
    "                del attention_mask\n",
    "                del startPositions\n",
    "                del endPositions\n",
    "                del outputs\n",
    "                del loss\n",
    "    outputName=(modelName+\"_\"+tokenizerName+\"_epoch-\"+str(EPOCH)).replace(\"/\",\"-\")\n",
    "    model.save_pretrained(\"./outputs/\"+outputName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daf02369",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80b498d2f4e4616b5380734be3430a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6624 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-2e9ab4a623cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-a0e9e293ffb8>\u001b[0m in \u001b[0;36mtrain_runner\u001b[1;34m(model, dataset, batch_size, num_train_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     24\u001b[0m                              end_positions=endPositions)\n\u001b[0;32m     25\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\a2\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\a2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#모델 훈련 실행\n",
    "#메모리 확보\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "train_runner(model,trainDataset, BATCH_SIZE, EPOCH, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778887a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
