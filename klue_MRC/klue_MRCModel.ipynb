{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedbf843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KLUE QA Dataset과 Roberta-large모델을 활용한 학습 예제 입니다.\n",
    "\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering,AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a80763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data를 읽어서 각 feature 별로 전처리하는 함수\n",
    "def readTrainData(path):\n",
    "    with open(path,'rb')as file:\n",
    "        MRCdata=json.load(file)\n",
    "    #data 구성\n",
    "    #context - 문장\n",
    "    #question - 질문\n",
    "    #answer - 정답\n",
    "    \n",
    "    contexts=list()\n",
    "    \n",
    "    questions=list()\n",
    "    answers=list()\n",
    "    \n",
    "    #하나의 문장에 여러 질문이 있을 수 있고, 질문이 여러개면 답변도 여러개이기 때문에 4중for문 사용\n",
    "    for item in tqdm(MRCdata[\"data\"]):\n",
    "        for passage in item['paragraphs']:\n",
    "            context=passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question=qa['question']\n",
    "                for ans in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(ans)\n",
    "    return contexts,questions,answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb9fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset에 정답의 끝나는 index도 추가하는 함수\n",
    "def endIdx(answers,contexts):\n",
    "    for answer,context in zip(answers,contexts):\n",
    "        ansText=answer['text']\n",
    "        startIdx=answer['answer_start']\n",
    "        endIdx=startIdx+len(ansText)\n",
    "        \n",
    "        answer['answer_end']=endIdx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d94ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이징, roberta-large사용\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82df87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KlueMRCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, contexts, questions, answers, modelMaxPositionEmbedings, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.answers = answers\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.model_max_position_embedings = modelMaxPositionEmbedings\n",
    "        self.encodings = self.tokenizer(self.contexts, \n",
    "                                        self.questions,\n",
    "                                        max_length=512,\n",
    "                                        truncation=True,\n",
    "                                        padding=\"max_length\",\n",
    "                                        return_token_type_ids=False)\n",
    "        self.addTokenPositions()\n",
    "        \n",
    "    def addTokenPositions(self):\n",
    "        startPositions = []\n",
    "        endPositions = []\n",
    "        for i in range(len(self.answers)):\n",
    "            startPositions.append(self.encodings.char_to_token(i, self.answers[i]['answer_start']))\n",
    "            endPositions.append(self.encodings.char_to_token(i, self.answers[i]['answer_end'] - 1))\n",
    "\n",
    "            # positions 값이 None 값이라면, answer가 포함된 context가 잘렸다는 의미\n",
    "            if startPositions[-1] is None:\n",
    "                startPositions[-1] = self.model_max_position_embedings\n",
    "            if endPositions[-1] is None:\n",
    "                endPositions[-1] = self.model_max_position_embedings\n",
    "\n",
    "        self.encodings.update({'startPositions': startPositions, 'endPositions': endPositions})\n",
    "\n",
    "        \n",
    "    def get_data(self):\n",
    "        return {\"contexts\":self.contexts, 'questions':self.questions, 'answers':self.answers}\n",
    "    \n",
    "    \n",
    "    def get_encodings(self):\n",
    "        return self.encodings\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key:torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec0a804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475f560dbad7470e8631ce08908d7a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12174 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#훈련 data 전처리\n",
    "contexts,questions,answers=readTrainData(\"./dataset/klue-mrc-v1.1_train.json\")\n",
    "endIdx(answers,contexts)\n",
    "trainDataset=KlueMRCDataset(contexts,questions,answers,512,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d31e44c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#model 가저오기\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6e32bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼파라미터 정의\n",
    "EPOCH=3\n",
    "LEARNING_RATE=5e-5\n",
    "BATCH_SIZE=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64c2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 훈련 실행함수(AdamW사용)\n",
    "#좀 더 다양한 dataset을 활용해 여러 모델을 만들 예정이므로 각 모델의 이름을 구별하기 위해 modelName변수를 활용\n",
    "def train_runner(model, dataset, batch_size, num_train_epochs, learning_rate,modelName):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "    global_total_step = len(train_dataloader) * num_train_epochs\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    with tqdm(total=global_total_step, unit='step') as t:\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "        for epoch in range(num_train_epochs):\n",
    "            for batch in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                startPositions = batch['startPositions'].to(device)\n",
    "                endPositions = batch['endPositions'].to(device)\n",
    "                outputs = model(input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             start_positions=startPositions,\n",
    "                             end_positions=endPositions)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_loss = loss.item() * len(input_ids)\n",
    "                total += len(input_ids)\n",
    "                total_loss += batch_loss\n",
    "                global_total_step += 1\n",
    "                t.set_postfix(loss=\"{:.6f}\".format(total_loss / total), batch_loss=\"{:.6f}\".format(batch_loss))\n",
    "                t.update(1)\n",
    "                \n",
    "                del input_ids\n",
    "                del attention_mask\n",
    "                del startPositions\n",
    "                del endPositions\n",
    "                del outputs\n",
    "                del loss\n",
    "    model.save_pretrained(\"./outputs/\"+modelName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf02369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DESKTOP\\anaconda3\\envs\\a2\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2347b3b5a22a4b08a93a423969db1e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6624 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#모델 훈련 실행\n",
    "train_runner(model,trainDataset, BATCH_SIZE, EPOCH, LEARNING_RATE, \"base_klue_data_model_epoch_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e201dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dev data를 읽어서 각 feature 별로 전처리하는 함수\n",
    "def readDevData(path):\n",
    "    with open(path,'rb')as file:\n",
    "        MRCdata=json.load(file)\n",
    "    #data 구성\n",
    "    #context - 문장\n",
    "    #question - 질문\n",
    "    #answer - 정답\n",
    "    \n",
    "    contexts=list()\n",
    "    \n",
    "    questions=list()\n",
    "    answers=list()\n",
    "    \n",
    "    #하나의 문장에 여러 질문이 있을 수 있고, 질문이 여러개면 답변도 여러개이기 때문에 4중for문 사용\n",
    "    for item in tqdm(MRCdata[\"data\"]):\n",
    "        for passage in item['paragraphs']:\n",
    "            context=passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question=qa['question']\n",
    "                tmpAnswer=list()\n",
    "                for ans in qa['answers']:\n",
    "                    tmpAnswer.append(answer['text'])\n",
    "                if len(tmpAnswer)!=0:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(ans)\n",
    "                    \n",
    "    return contexts,questions,answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalutate(predAnswers,realAnswers):\n",
    "    total=len(predAnswers)\n",
    "    correct=0\n",
    "    for pred,real in zip(predAnswers,realAnswers):\n",
    "        if pred in real:\n",
    "            correct+=1\n",
    "    return (correct/total)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_contexts, dev_questions, dev_answers = read_dev_klue(\"./klue_dataset/klue-mrc-v1.1_dev.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(contexts,questions):\n",
    "    device =torch.device('cuda') if torch.cuda is_available() else torch.device('cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    result=list()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for context, question in zip(contexts,questions):\n",
    "            encodings=tokenizer(context,question,max_length=512, truncation=True,\n",
    "                               padding=\"max_length\", return_token_type_ids=False)\n",
    "            encodings={key: torch.tensor([val]) for key, val in encodings,items()}\n",
    "            \n",
    "            input_ids=encodings[\"input_ids\"].to(device)\n",
    "            attention_mask=encodings[\"attention_mask\"]\n",
    "\n",
    "            outputs=model(input_ids,attention_mask=attention_mask)\n",
    "            start_logits, end_logits=outputs.start_logits, outputs.end_logits\n",
    "            token_start_index, token_end_index=start_logits.argmax(dim=-1),end_logits.argmax(dim=-1)\n",
    "            pred_ids=input_ids[0][token_start_index: token_end_index +1]\n",
    "            pred=tokenizer.decode(pred_ids)\n",
    "            result.append(pred)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_answers = prediction(dev_contexts, dev_questions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
